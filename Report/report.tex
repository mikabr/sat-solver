\documentclass[11pt]{article}
\usepackage[margin=3cm]{geometry}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage[hypcap]{caption}
\renewcommand*\abstractname{Summary}

\begin{document}
\setstretch{1.6}

\title{Solving Semantic Analogy Problems Using ConceptNet}
\author{Mika Braginsky and Will Whitney}
\maketitle

\begin{abstract}
We implemented a system for solving SAT analogy problems, using ConceptNet. The system attempts to find the relationship between the target pair of words (such as \emph{mason}:\emph{stone}) and each option pair (such as \emph{teacher}:\emph{chalk}, \emph{carpenter}:\emph{wood}, \emph{soldier}:\emph{gun}, \emph{photograph}:\emph{camera}, \emph{book}:\emph{word}), scores the similarity of each option's relationship to the target's relationship, and selects the option with the highest score. On a dataset of 374 questions, it achieves an accuracy rate of 29.7\%, with an accuracy rate of 47\% on the 99 questions for which it can obtain at least minimal information.
\end{abstract}

\section{Problem Overview}

Reasoning by analogy has long been recognized in the cognitive sciences as a key component of cognition, in that it forms the basis for processes such as decision making, object recognition, face perception, problem solving, and creativity. Modeling this type of reasoning and enabling computer systems to make use of it should therefore be an important step in enabling artificial intelligence systems to emulate vital aspects of cognition.

As a way of exploring the more general phenomenon of reasoning by analogy, we implemented a system for solving analogy questions from the SAT (a standardized test used for college admissions). SAT analogy questions are specifically designed to test students' skill in solving problems using analogy-driven logic, so they are a reasonable test base for evaluating the ability of a computer system to reason by analogy.

An SAT analogy question consists of a target pair of words and five option pairs of words. The task is to select the option pair that ``best expresses a relationship similar to that expressed in the original pair'', as stated in the test's directions. For example:

\texttt{
ostrich:bird\\
(a) lion:cat\\
(b) goose:flock\\
(c) ewe:sheep\\
(d) cub:bear\\
(e) primate:monkey}

To solve the problem, one must determine the relationship between each word pair:

\texttt{
An OSTRICH is a species of BIRD\\
(a) LION is a species of CAT\\
(b) FLOCK is a group of GOOSE\\
(c) EWE is a female SHEEP\\
(d) CUB is a young BEAR\\
(e) MONKEY is a  sub-group of PRIMATE}

Solving the problem requires a lot of semantic knowledge about how concepts are related to each other. In this question, the solver needs to not only know what ostriches and birds and lion and cats are, but also how ostriches are related to birds, how lions are realted to cats, etc. This knowledge must be also capture a rather specific level of detail -- if the solver only knew that an ostrich is a type of bird, it would conclude that the same relation holds not just between lion and cat, but also between ewe and sheep and between cub and bear.

In general, the solver must make use of extensive information about the relationships between concepts and make inferences about the similarities between relationships. Achieving competence on this task, then, is an indicator of considerable mastery of reasoning abilities and common sense knowledge, both for a human and a computer system.

\section{Previous Work}

A number of algorithms have been devised to approach this problem (Table~\ref{comparison} shows their accuracy on a common test of 374 questions). Broadly speaking, they fall into two categories: lexicon-based and corpus-based (some are hybrid, a mix of both). The lexicon-based algorithms tend to use information from WordNet, a database of English words that encodes many lexical and some semantic relationships. The corpus-based algorithms use a corpus of questions to train their algorithm for the problem.

While some of the previous algorithms achieve almost human-level performance, they don't faithfully capture the nature of a human approach to solving the problem. Humans doesn't need to be ``trained'' on hundreds of questions before being able to answer further ones, and humans utilize much more complex and extensive semantic knowledge than can be provided by WordNet.

\vspace*{1em}

\setstretch{1}
\begin{table}[h!]

\centering
\begin{tabular}{| l | c | c |}
\hline
\textbf{Reference for algorithm} & \textbf{Type} & \textbf{Correct} \\ \hline
\textit{Random guessing} & \textit{Random} & \textit{20.0\%} \\
Jiang and Conrath (1997) & Hybrid & 27.3\% \\
Lin (1998) & Hybrid & 27.3\% \\
Leacock and Chodrow (1998) & Lexicon-based & 31.3\% \\
Hirst and St.-Onge (1998) & Lexicon-based & 32.1\% \\
Resnik (1995) & Hybrid & 33.2\% \\
Turney (2001) & Corpus-based & 35.0\% \\
Mangalath et al. (2004) & Corpus-based & 42.0\% \\
Veale (2004) & Lexicon-based & 43.0\% \\
Bicici and Yuret (2006) & Corpus-based & 44.0\% \\
Herdağdelen and Baroni (2009) & Corpus-based & 44.1\% \\
Turney and Littman (2005) & Corpus-based & 47.1\% \\
Turney (2012) & Corpus-based & 51.1\% \\
Bollegala et al. (2009) & Corpus-based & 51.1\% \\
Turney (2008) & Corpus-based & 52.1\% \\
Turney (2006a) & Corpus-based & 53.5\% \\
Turney (2006b) & Corpus-based & 56.1\% \\
\textit{Average US college applicant} & \textit{Human} & \textit{57.0\%} \\ \hline
\end{tabular}

\caption[foo]{Results of various algorithms on a common 374 question data set.\footnotemark}
\label{comparison}

\end{table}

\footnotetext{\url{http://aclweb.org/aclwiki/index.php?title=SAT_Analogy_Questions_\%28State_of_the_art\%29}}

\setstretch{1.6}

\section{Approach}

Attempting to solve analogy problems in a more plausibly human-like way requires access to a great deal of semantic information. Our approach was to obtain this information using ConceptNet, a semantic network that attempts to encode ``common sense'' knowledge and other information that computers should know about the world\footnote{ConceptNet is developed by the Digital Intuition group, MIT Media Lab.}.

ConceptNet is a directed graph where the nodes are concepts and the edges are relationships between concepts. Its knowledge is assembled from a variety of sources, such as Wikipedia, Wiktionary, WordNet, and direct human contributions. It contains basic semantic relationships and everyday knowledge, such as:

\verb|dog| ---\verb|IsA|$\rightarrow$ \verb|pet|\\
\verb|learn| ---\verb|MotivatedByGoal|$\rightarrow$ \verb|know more information|

as well as more specialized cultural and scientific knowledge:

\verb|saxophone| ---\verb|UsedFor|$\rightarrow$ \verb|jazz|\\
\verb|plutinos| ---\verb|can cross|$\rightarrow$ \verb|Neptune's orbit|

If ConceptNet were identical to the full space of concepts and relationships known to a human, using its knowledge would allow a system to solve analogy questions as well as human. Such a system would determine the relationship between each word pair by finding the path from one word to the other, and then select an answer by picking the option with the same path (relationship) as the target pair.

However, ConceptNet is far from perfect -- in trying to find the relationship between two concepts, ConceptNet has both not enough data and noisy data. On one hand, it has not enough data in that it just doesn't know enough about some concepts, such that finding a path between them is impossible. At the same time, its relationships are often very broad, so for some concepts it finds many paths between them. For example, for the concepts ``note'' and ``music'', it finds that:

\verb|music| ---\verb|have or involve|$\rightarrow$ \verb|note|\\
\verb|note| ---\verb|RelatedTo|$\rightarrow$ \verb|music|\\
\verb|music| ---\verb|HasProperty|$\rightarrow$ \verb|note|\\
\verb|note| ---\verb|in|$\rightarrow$ \verb|music|\\

In light of noisy and insufficient data, our approach was to:

\begin{itemize}
\item Find the list of paths for the target pair
\item Find the list of paths for each option pair
\item For each option, compare the option's list of paths to the target's list of paths.\\ Score their similarity using a scoring function.
\item Pick the option with the highest similarity score
\end{itemize}

The closer that ConceptNet's knowledge is to a human's knowledge, the more similar this algorithm is to a human's problem solving approach.

\section{Implementation}
\textit{TODO: explain our imeplementation: queries to ConceptNet, parallelization, finding paths, similarity metric}

\section{Results}
\textit{TODO: show our results, discuss error types}

\section{Further Work}
\textit{TODO: give options of ways this could be improved/extended}

\clearpage

\section*{References}

Bicici, E., and Yuret, D. (2006). Clustering word pairs to answer analogy questions. Proceedings of the Fifteenth Turkish Symposium on Artificial Intelligence and Neural Networks (TAINN 2006).

Bollegala D., Matsuo Y., and Ishizuka M. (2009). Measuring the similarity between implicit semantic relations from the web. Proceedings of the 18th International Conference on World Wide Web, ACM, pages 651–660.

Herdağdelen A. and Baroni M. (2009) BagPack: A general framework to represent semantic relations. Proceedings of the EACL 2009 Geometrical Models for Natural Language Semantics (GEMS) Workshop, East Stroudsburg PA: ACL, 33-40.

Hirst, G., and St-Onge, D. (1998). Lexical chains as representation of context for the detection and correction of malapropisms. In C. Fellbaum (ed.), WordNet: An Electronic Lexical Database. Cambridge: MIT Press, 305-332.

Jiang, J.J., and Conrath, D.W. (1997). Semantic similarity based on corpus statistics and lexical taxonomy. Proceedings of the International Conference on Research in Computational Linguistics, Taiwan.

Leacock, C., and Chodorow, M. (1998). Combining local context and WordNet similarity for word sense identification. In C. Fellbaum (ed.), WordNet: An Electronic Lexical Database. Cambridge: MIT Press, pp. 265-283.

Lin, D. (1998). An information-theoretic definition of similarity. Proceedings of the 15th International Conference on Machine Learning (ICML-98), Madison, WI, pp. 296-304.

Mangalath, P., Quesada, J., and Kintsch, W. (2004). Analogy-making as predication using relational information and LSA vectors. In K.D. Forbus, D. Gentner \& T. Regier (Eds.), Proceedings of the 26th Annual Meeting of the Cognitive Science Society. Chicago: Lawrence Erlbaum Associates.

Resnik, P. (1995). Using information content to evaluate semantic similarity. Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI-95), Montreal, pp. 448-453.

Turney, P.D., Littman, M.L., Bigham, J., and Shnayder, V. (2003). Combining independent modules to solve multiple-choice synonym and analogy problems. Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-03), Borovets, Bulgaria, pp. 482-489.

Turney, P.D., and Littman, M.L. (2005). Corpus-based learning of analogies and semantic relations. Machine Learning, 60 (1-3), 251-278.

Turney, P.D. (2001). Mining the Web for synonyms: PMI-IR versus LSA on TOEFL. Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001), Freiburg, Germany, pp. 491-502.

Turney, P.D. (2006a). Expressing implicit semantic relations without supervision. Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (Coling/ACL-06), Sydney, Australia, pp. 313-320.

Turney, P.D. (2006b). Similarity of semantic relations. Computational Linguistics, 32 (3), 379-416.

Turney, P.D. (2008). A uniform approach to analogies, synonyms, antonyms, and associations. Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), Manchester, UK, pp. 905-912.

Turney, P.D. (2012). Domain and function: A dual-space model of semantic relations and compositions, Journal of Artificial Intelligence Research (JAIR), 44, 533-585.

Veale, T. (2004). WordNet sits the SAT: A knowledge-based approach to lexical analogy. Proceedings of the 16th European Conference on Artificial Intelligence (ECAI 2004), pp. 606–612, Valencia, Spain.

\end{document}
