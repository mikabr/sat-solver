\documentclass[11pt]{article}
\usepackage[margin=3cm]{geometry}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage[hypcap]{caption}
\renewcommand*\abstractname{Summary}

\begin{document}
\setstretch{1.6}

\title{Solving Semantic Analogy Problems Using ConceptNet}
\author{Mika Braginsky and Will Whitney}
\maketitle

\begin{abstract}
We implemented a system for solving SAT analogy problems, using ConceptNet. The system attempts to find the relationship between the target pair of words (such as \emph{mason}:\emph{stone}) and each option pair (such as \emph{teacher}:\emph{chalk}, \emph{carpenter}:\emph{wood}, \emph{soldier}:\emph{gun}, \emph{photograph}:\emph{camera}, \emph{book}:\emph{word}), scores the similarity of each option's relationship to the target's relationship, and selects the option with the highest score. On a dataset of 374 questions, it achieves an accuracy rate of 29.7\%, with an accuracy rate of 47\% on the 99 questions for which it can obtain at least minimal information.
\end{abstract}

\section{Problem Overview}

Reasoning by analogy has long been recognized in the cognitive sciences as a key component of cognition, in that it forms the basis for processes such as decision making, object recognition, face perception, problem solving, and creativity. Modeling this type of reasoning and enabling computer systems to make use of it should therefore be an important step in enabling artificial intelligence systems to emulate vital aspects of cognition.

As a way of exploring the more general phenomenon of reasoning by analogy, we implemented a system for solving analogy questions from the SAT (a standardized test used for college admissions). SAT analogy questions are specifically designed to test students' skill in solving problems using analogy-driven logic, so they are a reasonable test base for evaluating the ability of a computer system to reason by analogy.

An SAT analogy question consists of a target pair of words and five option pairs of words. The task is to select the option pair that ``best expresses a relationship similar to that expressed in the original pair'', as stated in the test's directions. For example:

\texttt{
ostrich:bird\\
(a) lion:cat\\
(b) goose:flock\\
(c) ewe:sheep\\
(d) cub:bear\\
(e) primate:monkey}

To solve the problem, one must determine the relationship between each word pair:

\texttt{
An OSTRICH is a species of BIRD\\
(a) LION is a species of CAT\\
(b) FLOCK is a group of GOOSE\\
(c) EWE is a female SHEEP\\
(d) CUB is a young BEAR\\
(e) MONKEY is a  sub-group of PRIMATE}

Solving the problem requires a lot of semantic knowledge about how concepts are related to each other. In this question, the solver needs to not only know what ostriches and birds and lion and cats are, but also how ostriches are related to birds, how lions are realted to cats, etc. This knowledge must be also capture a rather specific level of detail -- if the solver only knew that an ostrich is a type of bird, it would conclude that the same relation holds not just between lion and cat, but also between ewe and sheep and between cub and bear.

In general, the solver must make use of extensive information about the relationships between concepts and make inferences about the similarities between relationships. Achieving competence on this task, then, is an indicator of considerable mastery of reasoning abilities and common sense knowledge, both for a human and a computer system.

\section{Previous Work}

A number of algorithms have been devised to approach this problem (Table~\ref{comparison} shows their accuracy on a common test of 374 questions). Broadly speaking, they fall into two categories -- lexicon-based and corpus-based (some are hybrid, a mix of both). The lexicon-based algorithms tend to use information from WordNet, a database of English words that encodes many lexical and some semantic relationships. The corpus-based algorithms use a corpus of questions to train their algorithm for the problem.

While some of the previous algorithms achieve almost-human performance, they don't faithfully capture the nature of a human approach to solving the problem. A human doesn't need to be ``trained'' on hundreds of questions before being able to answer further ones, and human utilizes more complex and extensive semantic knowledge than can be provided by WordNet.

\vspace*{1em}

\setstretch{1}
\begin{table}[h!]

\centering
\begin{tabular}{| l | c | c |}
\hline
\textbf{Reference for algorithm} & \textbf{Type} & \textbf{Correct} \\ \hline
\textit{Random guessing} & \textit{Random} & \textit{20.0\%} \\
Jiang and Conrath (1997) & Hybrid & 27.3\% \\
Lin (1998) & Hybrid & 27.3\% \\
Leacock and Chodrow (1998) & Lexicon-based & 31.3\% \\
Hirst and St.-Onge (1998) & Lexicon-based & 32.1\% \\
Resnik (1995) & Hybrid & 33.2\% \\
Turney (2001) & Corpus-based & 35.0\% \\
Mangalath et al. (2004) & Corpus-based & 42.0\% \\
Veale (2004) & Lexicon-based & 43.0\% \\
Bicici and Yuret (2006) & Corpus-based & 44.0\% \\
HerdaÄŸdelen and Baroni (2009) & Corpus-based & 44.1\% \\
Turney and Littman (2005) & Corpus-based & 47.1\% \\
Turney (2012) & Corpus-based & 51.1\% \\
Bollegala et al. (2009) & Corpus-based & 51.1\% \\
Turney (2008) & Corpus-based & 52.1\% \\
Turney (2006a) & Corpus-based & 53.5\% \\
Turney (2006b) & Corpus-based & 56.1\% \\
\textit{Average US college applicant} & \textit{Human} & \textit{57.0\%} \\ \hline
\end{tabular}

\caption[foo]{Results of various algorithms on a common 374 question data set.\footnotemark}
\label{comparison}

\end{table}

\footnotetext{\url{http://aclweb.org/aclwiki/index.php?title=SAT_Analogy_Questions_\%28State_of_the_art\%29}}

\setstretch{1.6}

\section{Approach}
\textit{TODO: explain our approach, connect to human strategies, describe ConceptNet}

\section{Implementation}
\textit{TODO: explain our imeplementation: queries to ConceptNet, parallelization, finding paths, similarity metric}

\section{Results}
\textit{TODO: show our results, discuss error types}

\section{Further Work}
\textit{TODO: give options of ways this could be improved/extended}

\end{document}
